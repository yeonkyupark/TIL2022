# TIL20220302

### TIL20220316

#### 페널티 회귀분석 - 1

**페널티 회귀분석(penalized regression analysis)**[^mar-02-1](https://www.youtube.com/watch?v=3OEwk2VxZdE\&list=PLY0OaF78qqGAxKX91WuRigHpwBU0C2SB\_\&index=31\&t=228s)

* 릿지 (Ridge)\
  회귀계수를 0에 가깝게 만듬(모든 변수 사용), 독립변수의 회귀계수가 비슷한 경우 우수
* 라소 (Lasso)\
  설명력에 기여하지 못하는 독립변수의 회귀계수를 0으로 만듬(간명한 모델), 독립변수의 회귀변수간 차이가 클 때 우수
* 일래스틱넷 (ElasticNet), 릿지 + 라소

지나치게많은 독립변수를 갖는 모델에 페널티를 부과하는 방식으로 보다 간명한 회귀모델을 생성할 수 있다. 모델의 성능에 크게 기여하지 못하는 변수의 영향력을 축소(Lasso)하거나 모델에서 제거(Ridge)한다. 최소자승법에 의한 잔차(=관측값-예측값)의 제곱합과 페널티항의 합이 최소가 되도록 회귀계수를 추정한다.

```
str(Boston)
```

`glmnet` 패키지 내 `glmnet()` 함수를 통해 페널티 회귀분석을 수행할 수 있다. glmnet() 함수는 인자로 결과변수 y는 벡터로 예측변수 x는 행렬 형태로 제공해야 한다. 추가로 연속형 변수만 처리 가능하므로 범주형 변수는 사전에 더미변수로 변환해야 한다.

* family = 결과변수의 확률분포, gaussian, binomial 등
* alpha = 0(Ridge), 1(Lasso), 0\~1(ElasticNet)
* lambda = 패널티 크기 조절, 예측오차를 최소화 하는 람다 설정, 교차검정을 통해 설정

**Ridge Regression**

```
# 최적의 람다 계산
set.seed(123)
Boston.cv <- cv.glmnet(x = x, y = y, family = "gaussian", alpha = 0) # Ridge
plot(Boston.cv)
```

왼쪽의 점선은 최적 람다의 로그 값, 상단의 숫자는 예측변수의 개수, Ridge 회귀분석은 회귀계수를 0으로 만들지 않기 때문에 예측변수 개수가 줄어들지 않는다.

```
str(Boston.cv)
```

```
cbind(lambda.min = Boston.cv$lambda.min, lambda.min.log = log(Boston.cv$lambda.min))
```

```
Boston.gnet <- glmnet(x = x, y = y, family = "gaussian", 
       alpha = 0,
       lambda = Boston.cv$lambda.min)
coef(Boston.gnet)
```

```
Boston.pred <- predict(Boston.gnet, newx = Boston.test.x)
head(Boston.pred)
```

```
postResample(pred = Boston.pred, obs = Boston.test.y)
```

RMSE와 MAE는 오차 지표로 값이 작을수록, Rsquared는 모델의 설명력 지표로 값이 클수록 우수한 모델이다.

#### Markdown 문법

[https://daringfireball.net/projects/markdown/syntax#p](https://daringfireball.net/projects/markdown/syntax#p)

### TIL20220317

#### 분산분석

분산분석(analysis of variance, ANOVA)은 여러 모집단 간의 평균의 동일성을 검정한다.[^mar-02-2](https://www.youtube.com/watch?v=BnFkVjjSv6I\&list=PLY0OaF78qqGAxKX91WuRigHpwBU0C2SB\_\&index=11)

* 일원분산분석 (one-way ANOVA)
* 이원분산분석 (two-way ANOVA)
* 공분산분석 (analysis of corvariance, ANCOVA)
* 반복측정 분산분석 (repeated measures ANOVA)
* 다변량 분산분석 (multivariate analysis of variance, MANOVA)
* 다변량 공분산분석 (multivariate analysis of covariance, MANCOVA)

#### 페널티 회귀분석 - 2

```
str(Boston)
```

```
set.seed(123)
train.index <- createDataPartition(y = Boston$medv, p = 0.7, list = F)
Boston.train <- Boston[train.index, ]
Boston.test <- Boston[-train.index, ]
Boston.test.x <- model.matrix(medv ~ ., Boston.test)[, -1]
Boston.test.y <- Boston.test$medv

Boston.split <-  rbind(Train.Data = nrow(Boston.train), Test.Data = nrow(Boston.test))
colnames(Boston.split) <- c("Number"); Boston.split
```

`glmnet` 패키지 내 `glmnet()` 함수를 통해 페널티 회귀분석을 수행할 수 있다. glmnet() 함수는 인자로 결과변수 y는 벡터로 예측변수 x는 행렬 형태로 제공해야 한다. 추가로 연속형 변수만 처리 가능하므로 범주형 변수는 사전에 더미변수로 변환해야 한다.

* family = 결과변수의 확률분포, gaussian, binomial 등
* alpha = 0(Ridge), 1(Lasso), 0\~1(ElasticNet)
* lambda = 패널티 크기 조절, 예측오차를 최소화 하는 람다 설정, 교차검정을 통해 설정

```
if(!require(glmnet)) { install.packages("glmnet"); library(glmnet); }

# 예측변수 행렬 + 더미변수 생성
x <- model.matrix(medv ~., Boston.train)[, -1] # 불필요한 첫 번째 열 삭제
head(x)
```

```
# 결과변수
y <- Boston.train$medv
```

**Lasso Regression**

```
set.seed(123)
Boston.cv <- cv.glmnet(x = x, y = y, family = "gaussian", alpha = 1) # Lasso
plot(Boston.cv)
```

왼쪽의 점선은 예측오차를 최소화하는, 즉 예측 정확도를 가장 크게하는 로그 람다값을 나타낸다. Lasso 회귀분석에서는 영향력이 작은 예측변수의 회귀계수를 0으로 만들어 제거할 수 있다. 우측 상단의 예측변수 개수에서 확인이 가능하다.

```
cbind(lambda.min = Boston.cv$lambda.min, lambda.min.log = log(Boston.cv$lambda.min))
```

예측 정확도와 모델 간명도를 고려하여 최소 예측 오차의 1개 표준편차 이내에 있으면서 예측변수의 개수를 최소화하는 람다를 제공한다.

```
cbind(lambda.1se = Boston.cv$lambda.1se, lambda.1se.log = log(Boston.cv$lambda.1se))
```

```
cbind(lambda.min = coef(Boston.cv, Boston.cv$lambda.min), lambda.1se = coef(Boston.cv, Boston.cv$lambda.1se))
```

lambda.min은 1개의, lambda.1se는 3개의 예측변수가 제거되었다.

```
Boston.gnet1 <- glmnet(x = x, y = y, family = "gaussian", 
       alpha = 1,
       lambda = Boston.cv$lambda.min)
Boston.pred1 <- predict(Boston.gnet1, newx = Boston.test.x)
postResample(pred = Boston.pred1, obs = Boston.test.y)
```

```
Boston.gnet2 <- glmnet(x = x, y = y, family = "gaussian", 
       alpha = 1,
       lambda = Boston.cv$lambda.1se)
Boston.pred2 <- predict(Boston.gnet2, newx = Boston.test.x)
postResample(pred = Boston.pred2, obs = Boston.test.y)
```

**ElasticNet Regression**

ElasticNet읜 L2-norm, L1-norm 페널티항을 설정해야 하므로 최적의 alpha값을 산출해야 한다. _caret_ 패키지 내 _train()_ 함수를 이용한다.

```
set.seed(123)
Boston.cv <- train(form = medv ~ ., data = Boston.train, method = "glmnet",
                   trControl = trainControl(method = "cv",
                                            number = 10),
                   tuneLength = 10)

Boston.cv$bestTune
```

```
Boston.gnet <- glmnet(x = x, y = y, family = "gaussian", 
       alpha = Boston.cv$bestTune$alpha, 
       lambda = Boston.cv$bestTune$lambda)
coef(Boston.gnet)
```

```
Boston.pred <- predict(Boston.gnet, newx = Boston.test.x)
postResample(pred = Boston.pred, obs = Boston.test.y)
```

#### 모델별 비교

```
lambda <- 10^seq(-1, 5, length = 100)

# ridge
set.seed(123)
ridge <- train(form = medv ~ ., data = Boston.train, method = "glmnet",
               trControl = trainControl(method = "cv",
                                        number = 10),
               tuneGrid = expand.grid(alpha = 0, lambda = lambda))
coef(ridge$finalModel, ridge$bestTune$lambda)
ridge.pred <- predict(ridge, Boston.test)
postResample(pred = ridge.pred, obs = Boston.test.y)
```

```
# lasso
set.seed(123)
lasso <- train(form = medv ~ ., data = Boston.train, method = "glmnet",
               trControl = trainControl(method = "cv",
                                        number = 10),
               tuneGrid = expand.grid(alpha = 1, lambda = lambda))
coef(lasso$finalModel, lasso$bestTune$lambda)
lasso.pred <- predict(lasso, Boston.test)
postResample(pred = lasso.pred, obs = Boston.test.y)
```

```
# lasso
set.seed(123)
elastic <- train(form = medv ~ ., data = Boston.train, method = "glmnet",
               trControl = trainControl(method = "cv",
                                        number = 10),
               tuneLength = 10)
coef(elastic$finalModel, elastic$bestTune$lambda)
elastic.pred <- predict(elastic, Boston.test)
postResample(pred = elastic.pred, obs = Boston.test.y)
```

```
models <- list(ridge = ridge, lasso = lasso, elastic = elastic)
summary(resamples(models))
```

세 모델이 RMSE 관점에서 비슷한 성능을 보이는 것 같다. 통계적으로 유의한 차이가 있는지 검정해 본다.

```
summary(diff(resamples(models), metric="RMSE"))
```

행렬의 대각선 위쪽은 모델간 차이, 아래쪽은 유의확률을 나타낸다. 세 모델 간 통계적 유의한 차이는 없는 것으로 확인된다. 따라서 간명도 관점에서 예측변수의 개수가 적은 모델을 선택하는 것이 바람직하다.

### TIL20220318

#### 구미시 선별진료소

![구미시 선별진료소^mar-02-3](https://user-images.githubusercontent.com/72383349/158912378-c9df3d5a-61ab-4706-ae9f-a8bd98c07763.png)

### TIL20220319

#### 일반선형델회귀모델

일반선형회귀모델

: $f(\mu \_y) = \beta\_0 + \beta\_1 x\_1 + \beta\_2 x\_2 + ... + \beta\_n x\_n$

로지스틱 선형회귀모델

: $ln( \frac{p}{1-p} ) = \beta\_0 + \beta\_1 x\_1 + \beta\_2 x\_2 + ... + \beta\_m x\_m$

포아송 선형회귀모델

: $ln(\lambda) = \beta\_0 + \beta\_1 x\_1 + \beta\_2 x\_2 + ... + \beta\_m x\_m$

#### 이항 로지스틱 회귀분석

결과변수(종속변수)가 이분형 범주를 가질 때 예측변수(독립변수)로부터 결과변수의 범주를 예측한다.

```
if(!require(modeldata)) { install.packages("modeldata"); library(modeldata); }
data(mlc_churn)
str(mlc_churn)
```

```
churn <- mlc_churn[-c(1,3)]
churn$churn <- factor(ifelse(churn$churn=="no", 1, 2),
                      levels=c(1,2),
                      labels=c("no", "yes"))
str(churn)
```

```
churn_train <- churn[1:3333,]
churn_test <- churn[3334:5000,]
rbind(ChurnTrain = prop.table(table(churn_train$churn)),
      ChurnTest = prop.table(table(churn_test$churn)))
```

```
churn_logit <- glm(churn ~ ., data = churn_train, family = binomial(link = "logit"))
summary(churn_logit)
```

```
exp(coef(churn_logit)) # 해석상의 용이를 위해 오즈비를 지수변환 함
```

로지스틱 회귀모델의 통계적 유의성은 summary() 함수를 통해 제공되지 않아 직접 계산을 한다. Null deviance와 Residual deviance를 통해 검정할 수 있다.

Deviance(이탈도)

: 모델의 적합도 정도를 나타내는 지표, 이탈도가 적을수록 우수한 모델임

Null deviance는 상수항만을 포함하는 모델이고, Residual deviance는 모든변수를 포함하는 모델이다. 예측변수를 포함될수록 설명력은 좋아지기 때문에 이탈도는 작아질 수밖에 없다. 예측변수에 의해 작아지는 정도가 통계적으로 유의한지를 검정한다. 이탈도는 $\chi^2$ 분포를 따르므로 $\chi^2$ 검정 통계량을 계산해서 검정한다.

```
pchisq(q=(2758.3-2158.7), df=(3332-3315), lower.tail = F)
```

```
pchisq(q = (churn_logit$null.deviance - churn_logit$deviance),
       df = (churn_logit$df.null - churn_logit$df.residual), lower.tail = F)
```

```
churn_logit_pred <- predict(churn_logit, newdata = churn_test, type = "response")
head(churn_logit_pred)
```

```
churn_logit_pred <- factor(churn_logit_pred > 0.5,
                           levels = c(FALSE, TRUE),
                           labels = c("no", "yes"))
head(churn_logit_pred)
```

```
table(churn_logit_pred)
```

```
table(churn_test$churn, churn_logit_pred, dnn=c("Acutal", "Predicted"))
```

```
mean(churn_test$churn == churn_logit_pred)
```

단계별 변수 선택을 통해 유의한 변수만 선택하만 보다 간결한 모델을 생성할 수 있다.

```
churn_logit2 <- step(churn_logit, trace = F)
summary(churn_logit2)
```

```
pchisq(q = (churn_logit2$null.deviance - churn_logit2$deviance),
       df = (churn_logit2$df.null - churn_logit2$df.residual), lower.tail = F)
```

특정 예측변수의 변화에 따른 결과변수의 변화를 보기 위해서는 다른 변수를 고정한 후 특정 변수만 변화를 시키면서 결과를 확인할 수 있다.

```
table(churn_test$number_customer_service_calls)
```

서비스센터 전화회수의 분포는 0 \~ 7번으로 구성되어 있다. 나머지 변수는 평균이나 가장 낮은 변주 유형으로 고정한 데이터셋을 생성한다.

```
testdata <- data.frame(
  number_customer_service_calls = c(0:7),
  international_plan = "no",
  voice_mail_plan ="no" ,
  number_vmail_messages = mean(churn_test$number_vmail_messages),
  total_day_charge = mean(churn_test$total_day_charge),
  total_eve_minutes = mean(churn_test$total_eve_minutes),
  total_night_charge = mean(churn_test$total_night_charge),
  total_intl_calls = mean(churn_test$total_intl_calls),
  total_intl_charge = mean(churn_test$total_intl_charge)
); testdata
```

```
testdata$prob <- predict(churn_logit2, newdata = testdata, type = "response")
testdata[c(1,10)]
```

이항 로지스틱분석은 과산포의 문제를 확인해야 한다. 결과변수의 실제 관측된 분산이 이항분포의 기대되는 분산보다 더 클 때 발생한다. 과산포는 표준오차를 왜곡시켜 회귀계수의 유의성 검정을 부정확하게 만들 위험성이 있다. 과산포 발생 시 _family_ 인수에 _quasibinomial()_ 함수를 적용한다.

과산포를 확인하는 방법은 이탈도와 자유도간의 비율을 살펴본다. 이탈도대 자유도의 비율이 1을 크게 상외하면 과산포를 의심한다.

```
deviance(churn_logit2) / df.residual(churn_logit2)
```

또는 \*binomial()\*와 _quasibinomial()_ 함수를 적용한 모델을 생성하고 통계적으로 유의성을 검정하는 방법도 있다.

```
fit.origin1 <- glm(churn ~ number_customer_service_calls +
                  international_plan +
                  voice_mail_plan +
                  number_vmail_messages +
                  total_day_charge +
                  total_eve_minutes +
                  total_night_charge +
                  total_intl_calls +
                  total_intl_charge, data = churn_train,
                  family = binomial(link = "logit"))
fit.origin2 <- glm(churn ~ number_customer_service_calls +
                  international_plan +
                  voice_mail_plan +
                  number_vmail_messages +
                  total_day_charge +
                  total_eve_minutes +
                  total_night_charge +
                  total_intl_calls +
                  total_intl_charge, data = churn_train,
                  family = quasibinomial(link = "logit"))
pchisq(summary(fit.origin2)$dispersion*fit.origin1$df.residual,
       fit.origin1$df.residual, lower.tail = F)
```

위 예제에서는 p값이 0.08385493로 유의수준 0.05에서 통계적으로 과산포의 가능성은 작다고 볼 수 있다.

#### 패널티 로지스틱회귀분석

@ref(til20220316) 참고

당뇨평(diabetes) 여부를 예측하는 모델을 생성한다.

```
if(!require(mlbench)) { install.packages("mlbench"); library(mlbench); }
data("PimaIndiansDiabetes2")
str(PimaIndiansDiabetes2)
```

```
# 결측치 제거하는 전처리
PimaIndiansDiabetes3 <- na.omit(PimaIndiansDiabetes2)

if(!require(caret)) { install.packages("caret"); library(caret); }
set.seed(123)
train.index <- createDataPartition(y = PimaIndiansDiabetes3$diabetes,
                                   p = 0.7, list = F)
diabetes.train <- PimaIndiansDiabetes3[train.index, ]
diabetes.test <- PimaIndiansDiabetes3[-train.index, ]

class.status <- rbind(
  Train = prop.table(table(diabetes.train$diabetes)),
  Test = prop.table(table(diabetes.test$diabetes))
); colnames(class.status) <- c("Negative", "Positive");
class.status
```

```
# 예측오차를 최소로 하는 최적의 lambda 계산
if(!require(glmnet)) { install.packages("glmnet"); library(glmnet); }
x <- model.matrix(diabetes ~ ., diabetes.train)[, -1]
y <- ifelse(diabetes.train$diabetes == "pos", 1, 0)
test.x <- model.matrix(diabetes ~ ., diabetes.test)[, -1]
test.y <- diabetes.test$diabetes
diabetes.cv <- cv.glmnet(x, y, family = "binomial", alpha = 1)  # lasso 회귀모델
diabetes.lambda <- cbind(lambda.min = diabetes.cv$lambda.min, lambda.1se = diabetes.cv$lambda.1se)
rownames(diabetes.lambda) <- c("value");
diabetes.lambda
```

```
cbind(lambda.min = coef(diabetes.cv, diabetes.cv$lambda.min),
      lambda.1se = coef(diabetes.cv, diabetes.cv$lambda.1se)
)
```

예측 오차를 최소로 하는 lambda를 사용하여 모델을 생성한 후 성능을 평가한다.

```
diabets.gnet.min <- glmnet(x, y, family = "binomial", alpha = 1,
                           lambda = diabetes.cv$lambda.min)
diabets.gnet.min.pred <- predict(diabets.gnet.min, test.x, type="response")
diabets.gnet.min.pred <- ifelse(diabets.gnet.min.pred > 0.5, "pos", "neg")
table(test.y, diabets.gnet.min.pred, dnn = c("Actual", "Predicted"))
```

```
mean(test.y == diabets.gnet.min.pred)
```

간명도를 고려한 1se lambda를 사용하여 모델을 생성한 후 성능을 평가한다.

```
diabets.gnet.1se <- glmnet(x, y, family = "binomial", alpha = 1,
                           lambda = diabetes.cv$lambda.1se)
diabets.gnet.1se.pred <- predict(diabets.gnet.1se, test.x, type="response")
diabets.gnet.1se.pred <- ifelse(diabets.gnet.1se.pred > 0.5, "pos", "neg")
table(test.y, diabets.gnet.1se.pred, dnn = c("Actual", "Predicted"))
```

```
mean(test.y == diabets.gnet.min.pred)
```

모든 예측변수를 사용한 이항 로지스틱회귀모델을 생성하고 성능을 평가한다.

```
diabetes.logit <- glm(diabetes ~ ., data = diabetes.train, 
                      family = binomial(link = "logit"))
diabetes.logit.pred <- predict(diabetes.logit, diabetes.test, type = "response")
diabetes.logit.pred <- ifelse(diabetes.logit.pred > 0.5, "pos", "neg")
table(diabetes.test$diabetes, diabetes.logit.pred, dnn = c("Actual", "Predicted"))
```

```
mean(diabetes.test$diabetes == diabetes.logit.pred)
```

최종 모델별 예측정확도는 다음과 같고 간명도 관점에서 1se lambda를 이용한 모델을 선택하는 것이 바람직 하다.

```
perf.stats <- rbind(
  lambda.min = mean(test.y == diabets.gnet.min.pred),
  lambda.1se = mean(test.y == diabets.gnet.1se.pred),
  predictor.all = mean(diabetes.test$diabetes == diabetes.logit.pred)
); colnames(perf.stats) <- c("Accuracy");
perf.stats
```

### TIL20220320

#### 다항 로지스틱회귀분석

세 개 이상의 범주를 갖는 결과변수의 사건발생확률을 예측한다.

```
if(!require(EffectStars)) { install.packages("EffectStars"); library(EffectStars); }
data(PID)
str(PID)
```

```
levels(PID$PID)
```

```
if(!require(VGAM)) { install.packages("VGAM"); library(VGAM); }
pid.mlogit <- vglm(PID ~ ., family = multinomial(), data = PID)
summary(pid.mlogit)
```

_vglm()_ 함수는 마지막 범주를 기준범주로 사용한다.

```
exp(coef(pid.mlogit))
```

```
pid.mlogit.pred <- fitted(pid.mlogit)
head(pid.mlogit.pred)
```

교육수준에 정치 성향에 미치는 영향을 보고 위해 교육수준은 변화시키고 나머지 예측변수는 고정된 데이터셋을 생성한다.

```
testdata <- data.frame(
  Education = c("low", "high"),
  TVnews = mean(PID$TVnews),
  Income = mean(PID$Income),
  Age = mean(PID$Age),
  Population = mean(PID$Population)
); testdata;
```

```
pid.mlogit.pred <- predict(pid.mlogit, newdata = testdata, type="response")
cbind(testdata, pid.mlogit.pred)
```

교육수준이 low -> higt로 변화하면 Democrat일 확률이 감소하고, Republican일 확률이 증가하는 것을 볼 수 있다. 하지만 교육수준의 변화와 상관없이 항상 Democrate으로 예측할 확률이 가장 높기 때문에 교육수준과 성치성향과의 명확한 관계가 있다고 판단하기는 어렵다. 이는 교육수준의 회귀계수가 통계적으로 유의하지 않다는 것을 의미하기도 한다.

소득수준을 기준으로 같은 분석을 진행한다.

```
range(PID$Income)
testdata <- data.frame(
  Education = rep("low", 5),
  TVnews = mean(PID$TVnews),
  Income = seq(20, 100, 20),
  Age = mean(PID$Age),
  Population = mean(PID$Population)
); testdata;
```

```
pid.mlogit.pred <- predict(pid.mlogit, newdata = testdata, type="response")
cbind(testdata, pid.mlogit.pred)
```

소득의 수준에 따른 정치성향이 다른 것을 알 수 있다.

```
if(!require(MASS)) { install.packages("MASS"); library(MASS); }
str(fgl) # 유리 조각에 대한 성분 
```

```
levels(fgl$type) # (6개의 유리 유형)
```

```
# 변수 값의 범위가 다양하여 표준화하는 전처리 진행
fgl.scaled <- cbind(scale(fgl[, 1:9]), fgl[10])

set.seed(123)
train <- sample(nrow(fgl), 0.7*nrow(fgl))
fgl.train <- fgl.scaled[train, ]
fgl.test <- fgl.scaled[-train, ]
rbind(Train = table(fgl.train$type), Test = table(fgl.train$type))
```

```
if(!require(nnet)) { install.packages("nnet"); library(nnet); }
fgl.mlogit <- multinom(type ~ ., data = fgl.train, trace = 0)
summary(fgl.mlogit)
```

_multinom()_ 함수는 첫 번째 범주를 기준 범주로 사용한다.

회귀계수에 대한 유의확률을 제공하지 않아 별도로 계산을 해야 한다. 회귀계수를 표준오차로 나눠서 _z_ 값을 계산하고 이 _z_ 값으로 유의확률을 계산한다.

```
z <- summary(fgl.mlogit)$coefficients/summary(fgl.mlogit)$standard.errors
p <- (1- pnorm(abs(z), 0, 1))*2
print(p, digit=3)
```

```
fgl.mlogit.pred <- predict(fgl.mlogit, fgl.test, type = "response")
head(fgl.mlogit.pred)
```

```
cbind(round(fgl.mlogit.pred, 3), fgl.test["type"])
```

```
fgl.mlogit.pred <- colnames(fgl.mlogit.pred)[max.col(fgl.mlogit.pred)]
head(fgl.mlogit.pred)
```

```
table(fgl.test$type, fgl.mlogit.pred, dnn=c("Actual", "Predicted"))
```

```
table(fgl.test$type, 
      factor(fgl.mlogit.pred, 
             levels=levels(fgl.test$type),
             labels=levels(fgl.test$type)),
      dnn=c("Actual", "Predicted"))
```

```
mean(fgl.test$type == fgl.mlogit.pred)
```

100번의 교차 검정을 통해 보다 안정적인 예측 정확도를 계산한다.

```
fgl.mlogit.cv <- numeric()
for(i in 1:100) {
  train <- sample(nrow(fgl), 0.7*nrow(fgl))
  fgl.train <- fgl.scaled[train, ]
  fgl.test <- fgl.scaled[-train, ]
  
  fgl.mlogit <- multinom(type ~ ., data = fgl.train, trace = 0)
  fgl.mlogit.pred <- predict(fgl.mlogit, fgl.test, type = "probs")
  fgl.mlogit.pred <- colnames(fgl.mlogit.pred)[max.col(fgl.mlogit.pred)]
  
  fgl.mlogit.cv[i] <- mean(fgl.test$type == fgl.mlogit.pred)
}
summary(fgl.mlogit.cv)
```

```
boxplot(fgl.mlogit.cv, horizontal = T, col="tomato",
        xlab="Accuracy", main = "Accuracy for Forensic Glass (100 samples)")
```

```
# fgl.mlogit <- vglm(type ~ ., family = multinomial(), data = fgl.train)
```

```
# fgl.mlogit.pred <- predict(fgl.mlogit, newdata = fgl.test, type="response")
# fgl.mlogit.pred <- colnames(fgl.mlogit.pred)[max.col(fgl.mlogit.pred)]
# mean(fgl.test$type == fgl.mlogit.pred)
```

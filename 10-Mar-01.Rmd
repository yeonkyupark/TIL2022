# 2022년 3월

```{r echo=FALSE, include=FALSE}
library(ggplot2)
library(MASS)
library(tidyr)
library(palmerpenguins)
library(dplyr)
library(mice)
library(tidyverse)
library(caret)
library(mlbench)
library(rpart)
```

## TIL20220316

### 페널티 회귀분석

**페널티 회귀분석(penalized regression analysis)**^[https://www.youtube.com/watch?v=3OEwk2VxZdE&list=PLY0OaF78qqGAxKX91WuRigHpwBU0C2SB_&index=31&t=228s]  

* 릿지 (Ridge)  
회귀계수를 0에 가깝게 만듬(모든 변수 사용), 독립변수의 회귀계수가 비슷한 경우 우수
* 라소 (Lasso)  
설명력에 기여하지 못하는 독립변수의 회귀계수를 0으로 만듬(간명한 모델), 독립변수의 회귀변수간 차이가 클 때 우수
* 일래스틱넷 (ElasticNet), 릿지 + 라소

지나치게많은 독립변수를 갖는 모델에 페널티를 부과하는 방식으로 보다 간명한 회귀모델을 생성할 수 있다. 모델의 성능에 크게 기여하지 못하는 변수의 영향력을 축소(Lasso)하거나 모델에서 제거(Ridge)한다. 최소자승법에 의한 잔차(=관측값-예측값)의 제곱합과 페널티항의 합이 최소가 되도록 회귀계수를 추정한다.  


```{r}
str(Boston)
```

```{r}
set.seed(123)
train.index <- createDataPartition(y = Boston$medv, p = 0.7, list = F)
Boston.train <- Boston[train.index, ]
Boston.test <- Boston[-train.index, ]
Boston.split <-  rbind(Train.Data = nrow(Boston.train), Test.Data = nrow(Boston.test))
colnames(Boston.split) <- c("Number"); Boston.split
```

`glmnet` 패키지 내 `glmnet()` 함수를 통해 페널티 회귀분석을 수행할 수 있다. glmnet() 함수는 인자로 결과변수 y는 벡터로 예측변수 x는 행렬 형태로 제공해야 한다. 추가로 연속형 변수만 처리 가능하므로 범주형 변수는 사전에 더미변수로 변환해야 한다. 

* family = 결과변수의 확률분포, gaussian, binomial 등
* alpha = 0(Ridge), 1(Lasso), 0~1(ElasticNet)
* lambda = 패널티 크기 조절, 예측오차를 최소화 하는 람다 설정, 교차검정을 통해 설정

```{r}
if (!require(glmnet)) {
  install.packages("glmnet")
  library(glmnet)
}

# 예측변수 행렬 + 더미변수 생성
x <- model.matrix(medv ~., Boston.train)[, -1] # 불필요한 첫 번째 열 삭제
head(x)
```

```{r}
# 결과변수
y <- Boston.train$medv
```

```{r}
# 최적의 람다 계산
set.seed(123)
Boston.cv <- cv.glmnet(x = x, y = y, family = "gaussian", alpha = 0) # Ridge
plot(Boston.cv)
```

왼쪽의 점선은 최적 람다의 로그 값, 상단의 숫자는 예측변수의 개수, Ridge 회귀분석은 회귀계수를 0으로 만들지 않기 때문에 예측변수 개수가 줄어들지 않는다.  

```{r}
str(Boston.cv)
```

```{r}
cbind(lambda.min = Boston.cv$lambda.min, lambda.min.log = log(Boston.cv$lambda.min))
```


```{r}
Boston.gnet <- glmnet(x = x, y = y, family = "gaussian", 
       alpha = 0, # Ridge
       lambda = Boston.cv$lambda.min)
coef(Boston.gnet)
```

```{r}
Boston.test.x <- model.matrix(medv ~ ., Boston.test)[, -1]
Boston.test.y <- Boston.test$medv
Boston.pred <- predict(Boston.gnet, newx = Boston.test.x)
head(Boston.pred)
```

```{r}
postResample(pred = Boston.pred, obs = Boston.test.y)
```

RMSE와 MAE는 오차 지표로 값이 작을수록, Rsquared는 모델의 설명력 지표로 값이 클수록 우수한 모델이다.  


### Markdown 문법

https://daringfireball.net/projects/markdown/syntax#p 
